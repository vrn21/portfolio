---
title: "Build an RL Environment End-to-End (With a Trading Case Study)"
publishedAt: "2026-02-21"
summary: "A practical guide to building RL environments end-to-end: package the world in Docker, expose an agent interface, define tasks, run evals, and grade reliably. Includes a trading simulator case study from this repo."
tags: "RL, Evaluation, Docker, Python"
---

# Build an RL Environment End-to-End (With a Trading Case Study)

If you’ve ever used a famous RL environment, you’ve already seen the same pattern:

- someone packaged a “world”
- exposed a repeatable interface
- defined a task distribution
- and built an evaluation loop that returns scores you can trust

This post shows how to build that, end-to-end, in a way you can reuse for almost any domain.

We’ll keep one dedicated section at the end for the example in this repo: `trading-rl-env/`.

{/*
FIGURE 1 (primary diagram slot): "Environment, Agent, Eval"
Swimlane: Agent (outside) <-> Environment (container) -> Grader -> Score.
Label: tasks/scenarios instantiate episodes; tool calls produce observations; grader produces reward.
*/}

## 1. What You’re Building (In Plain English)

An RL “environment” is not a Docker image, and it’s not a website.
It’s the thing that makes **episodes** possible.

An episode is one run where:

1. The environment starts from a known initial state.
2. The agent takes actions.
3. The environment returns observations.
4. The run terminates (success/failure/timeout/max steps).
5. You compute reward (often by grading final state).

If you build those mechanics cleanly, the specific domain becomes interchangeable.

### Glossary (terms you’ll see in most RL docs)

- **Environment**: the interactive system the agent is learning to operate in.
- **World / system under test**: the actual service/simulator/UI/exchange the environment wraps.
- **Action**: what the agent is allowed to do next.
- **Observation**: what the agent gets back after acting.
- **Episode**: one complete run from reset to termination.
- **Task**: a specific scenario instance (a scenario + concrete args).
- **Scenario**: a template for an episode (setup -> prompt -> evaluate).
- **Eval / evaluation**: running many tasks and collecting scores/trajectories.
- **Grader / evaluation function**: code that turns final state (and sometimes the trajectory) into a score.
- **Spec**: the contract for actions, observations, reset/termination, grading, and determinism.
- **Trajectory / trace**: the logged sequence of tool calls and observations (how the agent behaved).

{/*
FIGURE 2 (diagram slot): "Episode Lifecycle"
Setup/reset -> agent loop (action/observation) -> termination -> grading -> score in [0,1].
Call out two end conditions: terminated vs truncated/timeouts (common RL terminology).
*/}

## 2. One Practical Way To Build Environments: HUD + Templates

One path to building “real” RL environments is to use HUD’s platform and local templates.

- HUD website: [hud.ai](https://hud.ai)
- Docs referenced by templates in this repo: [docs.hud.ai](https://docs.hud.ai)

Why use a platform workflow at all?
Because most environment failures are not ML failures. They are engineering failures:

- the world doesn’t boot deterministically
- reset is leaky
- logs corrupt the protocol
- graders are gameable
- tasks are underspecified
- scores aren’t reproducible across versions

HUD’s templates push you into sane defaults for those problems.

### Sources of truth in this repo

These directories are the canonical references for how environments are built here:

- `hud-blank/` (minimal environment + scenarios)
- `hud-browser/` (browser/computer tools + app-based scenarios)
- `coding-template/` (task-driven coding evals + richer grading patterns)
- `trading-rl-env/` (multi-service simulator + domain tools + weighted graders)

{/*
FIGURE 3 (diagram slot): "Template Ladder"
A “difficulty ladder” showing hud-blank -> hud-browser/coding-template -> trading-rl-env.
Purpose: help readers choose a starting template.
*/}

## 3. The Components of an RL Environment (And What They Are Here)

To build an environment end-to-end, break it into components you can implement and test.

1. **Dockerfile**: packages the world and defines how it boots.
2. **Environment entrypoint (`env.py`)**: registers tools and scenarios.
3. **Tools (optional)**: the agent-facing action interface.
4. **Scenarios**: define the episode lifecycle (setup -> prompt -> evaluate).
5. **Tasks**: concrete scenario instances (often from JSON or platform datasets).
6. **Evals**: run tasks through an agent loop and record trajectories.
7. **Graders**: turn final state (and sometimes the trajectory) into a score.
8. **Tests**: prove each layer works before you involve an agent.

### What each component maps to in this repo

| Component | What it is | Trading env (`trading-rl-env/`) | Other templates |
|---|---|---|---|
| Dockerfile | World packaging + boot | `Dockerfile.hud`, `run.sh` | `hud-blank/Dockerfile.hud`, `hud-browser/Dockerfile.hud`, `coding-template/Dockerfile.hud` |
| env.py | Tools + scenarios wiring | `env.py` | `hud-blank/env.py`, `hud-browser/env.py`, `coding-template/env.py` |
| Tools | Actions + observations | `tools/` | `coding-template/tools/`, `hud-browser/tools/`, single tool in `hud-blank/env.py` |
| Scenarios | Episode templates | `tasks/take_profit.py` | `hud-blank/env.py`, `hud-browser/scenarios/`, `coding-template/tasks/` |
| Tasks | Scenario instances | `remote_tasks.json` | `hud-blank/remote_tasks.json`, `hud-browser/remote_tasks.json`, `coding-template/remote_tasks.json` |
| Evals | “Run N tasks” | `local_test.py`, `remote_test.py` | same pattern in templates |
| Graders | Scoring logic | `grading/spec.py`, `grading/graders.py` | richer patterns in `coding-template/grading/` |
| Tests | Layered validation | `test_full.py` | `local_test.py` scripts + template-specific checks |

{/*
FIGURE 4 (diagram slot): "Component Map"
Boxes for Dockerfile -> env.py -> tools/scenarios -> tasks -> eval loop -> grader -> score.
Overlay: tests as a layer that touches each box.
*/}

## 4. Step 1: Start With the Dockerfile (Package the World)

If your environment wraps a real system, the Docker image is the environment.
It contains everything the agent needs to interact with the world:

- system dependencies
- services (DB, simulator, app server)
- your environment code (`env.py`, tools, scenarios, graders)
- a deterministic startup sequence

### The Dockerfile’s job

- Install what the world needs.
- Copy in your environment code.
- Start any dependencies.
- Start the environment process (the MCP server) as the main process.

Concrete examples to study:

- Minimal backend + env process: `hud-blank/Dockerfile.hud`
- Browser stack + env process: `hud-browser/Dockerfile.hud`
- Multi-service simulator startup: `trading-rl-env/Dockerfile.hud` and `trading-rl-env/run.sh`

### stdout/stderr rule (non-negotiable)

In these templates, the environment communicates with the agent over a JSON-RPC protocol on stdout.
That means:

- stdout must be reserved for protocol
- logs must go to stderr

This is enforced explicitly in two places:

- Python logging configured to stderr (see `hud-blank/env.py`).
- Shell entrypoint redirects its own output to stderr, then restores stdout only when exec-ing into the env process (see `trading-rl-env/run.sh`).

{/*
FIGURE 5 (diagram slot): "Startup Timeline"
Container start -> start deps -> wait for health -> exec env process.
Call out why waiting is necessary (no tool calls until world is healthy).
*/}

## 5. Step 2: Wire the Environment Entrypoint (`env.py`)

Once the world boots, you need a single process that exposes the contract:

- tools: what the agent can do
- scenarios: what episodes look like
- lifecycle: initialization and cleanup

You’ll see the same shape across templates:

- Create `env = Environment(name=...)`
- Register tools (directly or via routers/modules)
- Register scenarios
- `env.run(transport="stdio")`

Sources of truth:

- `hud-blank/env.py` (tools + scenario + initialize/shutdown)
- `hud-browser/env.py` (includes routers + scenario registration helpers)
- `trading-rl-env/env.py` (wires client, portfolio, tools, scenarios)

{/*
FIGURE 6 (diagram slot): "Tool Call Data Path"
Agent -> tool call -> tool handler -> world (HTTP/FIX/files) -> tool response -> Agent.
Side note: trajectories/logs recorded for later debugging.
*/}

## 6. Step 3: Tools Are Optional (Start With a Minimal “Handset”)

Many environments do not need a large custom toolset.
You can get surprisingly far with a small generic set:

- a “computer” tool (screenshots + mouse/keyboard) for UI-driven tasks
- `bash` for shell-based tasks
- an editor/read/write tool for files

Those patterns exist in this repo:

- UI-first: `hud-browser/` (computer + browser tooling)
- Code-first: `coding-template/` (bash + editor tooling)
- Domain-first: `trading-rl-env/tools/` (trading-specific tools)

When should you add custom tools?

- when the world has a domain protocol (FIX, SQL, internal RPC)
- when you want safer actions than “arbitrary bash”
- when the observation surface must be structured and stable (JSON payloads)

When should you avoid custom tools?

- when a generic tool already provides the action surface
- when you can express the task purely through UI or filesystem operations

Practical guideline: start with the minimum toolset that makes tasks solvable, then expand only when tasks force you to.

{/*
FIGURE 7 (figure slot): "Minimal Toolset"
Three cards: computer, bash, editor. Purpose: normalize that custom tools are optional.
*/}

## 7. Step 4: Define Tasks (Scenarios -> Task Instances)

In these templates, a scenario is an episode template:

1. reset/setup
2. yield a prompt (agent acts)
3. evaluate final state
4. yield a score

Example to internalize:

- `hud-blank/env.py` shows the smallest possible scenario: reset -> prompt -> score.

### Tasks are scenario instances

A task is “scenario + arguments”.
In practice, tasks come from:

- code (create `env("scenario", **args)` objects)
- JSON (see `remote_tasks.json`)
- platform datasets (load by slug)

Concrete references:

- `hud-blank/remote_tasks.json`
- `hud-browser/remote_tasks.json`
- `coding-template/remote_tasks.json`
- `trading-rl-env/remote_tasks.json`

{/*
FIGURE 8 (figure slot): "Task JSON"
Show one task object and label: env name, scenario id, args.
Purpose: demystify how tasks travel through eval systems.
*/}

## 8. Step 5: Build and Deploy (Preferred: HUD Platform)

You can run environments locally, but if you want repeatable evals at scale, you eventually need deployment.

There are two deployment workflows that show up across these templates:

1. Preferred: connect the repo on the HUD platform (builds happen automatically on push)
2. CLI: build and deploy from your terminal (`hud build`, `hud deploy`)

{/*
FIGURE 9 (diagram slot): "Deploy Pipeline"
Repo push -> build image -> environment version -> tasks -> eval jobs -> scores + trajectories.
*/}

### Option A (preferred): Deploy via the HUD platform UI

All of the template READMEs follow the same platform flow:

1. Push your environment to GitHub.
2. Go to [hud.ai](https://hud.ai) and create a new Environment.
3. Connect your GitHub repo.
4. The platform builds your environment automatically on pushes.
5. Create tasks from your scenarios.
6. Run evals and inspect scores and trajectories.

Spaces for screenshots:

{/* SCREENSHOT 1: hud.ai "New Environment" (connect GitHub repo) */}
{/* SCREENSHOT 2: Build logs / build success status */}
{/* SCREENSHOT 3: Environment page showing the environment slug */}
{/* SCREENSHOT 4: "Create Task" from a scenario */}
{/* SCREENSHOT 5: Taskset page with tasks listed */}
{/* SCREENSHOT 6: Running job configuration (model, max steps, group size) */}
{/* SCREENSHOT 7: Results view (score distribution) */}
{/* SCREENSHOT 8: Trajectory viewer / trace replay */}

Concrete references for how the templates describe this flow:

- `hud-blank/README.md`
- `hud-browser/README.md`
- `coding-template/README.md`

### Option B: Deploy via CLI (`hud build`, `hud deploy`)

If you prefer terminal workflows, the coding template includes a canonical CLI reference:

- `coding-template/HUD_CLI_GUIDE.md`

At a high level:

```bash
# Build the Docker image (local)
hud build .

# Deploy (remote)
hud deploy .
```

Some environments require build args or secrets at deploy time. For example, the coding template documents a `REPO_URL` build arg:

```bash
hud deploy . --build-arg REPO_URL=https://github.com/your-org/your-repo
```

For the trading example in this repo, the README shows a directory-targeted deploy:

```bash
hud deploy trading-rl-env
```

Space for screenshots:

{/* SCREENSHOT 9: Terminal output of `hud deploy ...` (show env build + deploy success) */}

## 9. Step 6: What Happens After You Hand Tasks to an Agent (Evals)

Once you have a list of tasks, an evaluation run is mechanically simple:

1. Pick a task (scenario + args).
2. Start an episode (setup/reset).
3. Send the prompt to the agent.
4. The agent takes actions (tool calls) and receives observations (tool responses).
5. The episode terminates (success/failure/timeout/max steps).
6. Run the grader and produce a score.
7. Store the trajectory for debugging and iteration.

You can see the “local dev” and “platform eval” hooks in the templates:

- Local dev patterns show up in `local_test.py` scripts.
- Remote eval patterns show up in `remote_test.py` scripts and README sections.

If you only add one debugging feature to your environment, make it easy to inspect trajectories. Most issues become obvious once you can replay what the agent actually did.

{/*
FIGURE 10 (diagram slot): "Eval Flow"
Task -> episode -> tool loop -> termination -> grading -> score + trajectory.
Add a callout: “trajectory is the debugger”.
*/}

## 10. Step 7: Graders (Make Reward Trustworthy)

Grading is where environments either become credible or become noise.

The core principle:

- grade final state, not agent self-reporting

This repo uses a structured grading pattern with weighted subscores:

- `SubGrade`: individual graded component (score + weight + metadata)
- `Grade`: aggregated score with weight validation

Sources of truth:

- `trading-rl-env/grading/spec.py`
- `trading-rl-env/grading/graders.py`
- richer patterns: `coding-template/grading/`

### Partial credit beats binary when you can afford it

Binary scoring is fine for many benchmarks, but partial credit is often better for training signals.
That’s why weighted subscores show up frequently: they make it harder for “do nothing” and “almost correct” to collapse into the same signal.

{/*
FIGURE 11 (diagram slot): "Grade Composition"
PnLGrader (0.8) + TradeActivityGrader (0.2) -> Grade -> score.
Keep the diagram generic; swap in trading names as a caption in the case study.
*/}

## 11. Alpha: What Makes a Great Environment (Beyond “It Runs”)

Here’s the “alpha” that separates a demo from an environment you can actually iterate on:

- **Reset semantics are real**: every episode starts clean (no leaked DB rows, no stale caches, no leftover processes).
- **Determinism is intentional**: pin dependencies, control randomness, version changes like breaking API changes.
- **Observations are stable**: if you change keys/units/meaning, it’s a spec change.
- **Action space is safe**: narrow tools reduce reward hacking and accidental damage.
- **Reward is hard to game**: graders check real state and resist “shortcut” behaviors.
- **Trajectories are first-class**: you can debug by replaying what happened.
- **Tasks are a distribution**: you’re not building one task, you’re building a taskset with difficulty spread.
- **Documentation is part of the spec**: prompts, tool docs, and invariants live next to the code.

{/*
FIGURE 12 (diagram slot): "Environment Quality Checklist"
A single reference poster-style diagram containing the bullets above.
*/}

## 12. Case Study: How `trading-rl-env/` Implements the Components

This section is intentionally example-specific.
It exists to show how the generic steps map onto a real, non-toy system.

### 12.1 Directory Tour (What Lives Where)

At a glance, `trading-rl-env/` looks like this:

```text
trading-rl-env/
├── Dockerfile.hud          # packages the world + environment process
├── run.sh                  # deterministic boot + stdout hygiene
├── env.py                  # registers tools + scenarios; runs stdio JSON-RPC
├── backend/
│   └── client.py           # QuantReplay REST+FIX client + Portfolio state
├── tools/                  # agent-facing trading API (actions/observations)
├── tasks/                  # scenarios (episode templates)
├── grading/                # grade spec + graders (reward)
├── remote_tasks.json       # example task instances for remote eval
├── local_test.py           # local dev script (call tools / run a scenario)
└── test_full.py            # layered integration test
```

{/*
FIGURE 14 (figure slot): "Trading Env File Map"
A labeled file tree with 1-line callouts per directory (world, interface, scenarios, grading, tests).
*/}

### 12.2 The World (What Runs Inside the Container)

The “world” here is a realistic market simulator with two interfaces:

- QuantReplay REST API for venue status and symbol listings (`:9050`)
- QuantReplay FIX gateway for order entry and execution reports (`:9051`)

That world is backed by a PostgreSQL database that is baked into the image at build time:

- schema migrations are run during the Docker build
- demo market data is seeded during the Docker build

Sources of truth:

- `trading-rl-env/Dockerfile.hud`
- `trading-rl-env/run.sh`

{/*
FIGURE 13 (diagram slot): "Trading Architecture"
Boxes: Postgres (seeded at build) -> QuantReplay engine (REST :9050, FIX :9051) -> MCP server (stdio JSON-RPC).
Agent outside the container calls tools; tools call REST/FIX inside.
*/}

Spaces for screenshots (trading-specific):

{/* SCREENSHOT 10: curl /api/venuestatus returning healthy */}
{/* SCREENSHOT 11: curl /api/listings showing AMZ, etc. */}
{/* SCREENSHOT 12: netcat showing FIX port open (nc -vz localhost 9051) */}

### 12.3 Boot Sequence (Why `run.sh` Exists)

If you only run one process in your container, you can often get away with a simple `CMD`.
But this environment needs multiple dependencies to be ready before the agent can safely interact.

`trading-rl-env/run.sh` does three important things:

1. Starts Postgres and waits until `pg_isready` says the DB is ready.
2. Starts QuantReplay and waits until `GET /api/venuestatus` succeeds.
3. Preserves stdout for the JSON-RPC protocol:
   - entrypoint logs go to stderr
   - stdout is restored only when exec-ing into the env process

This is the “it works in production” detail that most environment writeups omit.

{/*
FIGURE 15 (diagram slot): "Boot Timeline"
Postgres start -> readiness -> QuantReplay start -> readiness -> exec `hud dev env:env --stdio`.
*/}

### 12.4 The Environment Process (`env.py`)

`trading-rl-env/env.py` is intentionally thin. It wires pieces together:

- creates `Environment("trading")`
- creates shared state: `QuantReplayClient()` and `Portfolio()`
- lifecycle hooks:
  - `@env.initialize`: checks REST health, establishes FIX connection
  - `@env.shutdown`: disconnects FIX, closes HTTP client
- registers the tool modules (`tools/market.py`, `tools/orders.py`, `tools/portfolio.py`)
- registers the scenario module (`tasks/take_profit.py`)
- runs `env.run(transport="stdio")`

Source of truth:

- `trading-rl-env/env.py`

{/*
FIGURE 16 (diagram slot): "Env Wiring"
Boxes: env.py -> registers tools -> registers scenarios -> talks to QuantReplayClient + Portfolio.
*/}

### 12.5 The Bridge to the World (`backend/client.py`)

This is where “environment engineering” becomes “systems engineering”.

`trading-rl-env/backend/client.py` contains two key pieces:

#### QuantReplayClient (world communication)

- REST:
  - `health_check()` hits `/api/venuestatus`
  - `get_listings()` hits `/api/listings` and parses `{"listings": [...]}` into a symbol list
  - `reset_venue()` exists for stronger reset semantics when/if you need to clear live venue state
- FIX:
  - `connect()` opens a TCP socket and performs a FIXT.1.1 logon handshake
  - `place_order(...)` sends `NewOrderSingle (35=D)` with the required tags (including `TransactTime (60)`)
  - `cancel_order(...)` sends `OrderCancelRequest (35=F)` with required tags
  - `poll_fills()` reads `ExecutionReport (35=8)` messages and extracts trade fills (`ExecType (150)=F`)

The client is configurable via env vars (useful for alternate deployment topologies):

- `QUANTREPLAY_REST_URL` (default `http://localhost:9050`)
- `QUANTREPLAY_FIX_HOST` (default `localhost`)
- `QUANTREPLAY_FIX_PORT` (default `9051`)

Source of truth:

- `trading-rl-env/backend/client.py`

#### Portfolio (episode state)

The environment tracks per-episode state in-memory:

- cash balance
- open positions with average cost
- fill history (symbol/side/qty/price)

That state feeds two things:

1. Observations (`get_portfolio()`, `get_last_price(...)`)
2. Grading (`PnLGrader` and `TradeActivityGrader`)

One deliberate tradeoff in this example: net profit uses the *last fill price* as the mark for any open position (because the environment doesn’t expose a live order book feed).

Source of truth:

- `trading-rl-env/backend/client.py`

{/*
FIGURE 17 (diagram slot): "State Model"
Portfolio: cash, positions, fills; arrows from fills -> last_price -> net_profit -> grader.
*/}

### 12.6 The Agent Interface (Tools)

The agent does not speak FIX or SQL directly. It calls tools.

The trading tool surface is intentionally small:

- `list_symbols()` (discover what’s tradable)
- `place_order(symbol, side, qty, price)` (submit a limit order)
- `cancel_order(order_id, symbol, side)` (cancel a resting order)
- `poll_fills()` (receive executions since last poll)
- `get_last_price(symbol)` (derived observation from fill history)
- `get_portfolio()` (cash/positions/net profit snapshot)

Sources of truth:

- `trading-rl-env/tools/market.py`
- `trading-rl-env/tools/orders.py`
- `trading-rl-env/tools/portfolio.py`

Two practical tool-design patterns worth copying:

1. Validate tool inputs early (side must be BUY/SELL, qty positive, price positive).
2. Return structured JSON consistently (stable keys like `order_id`, `fills`, `count`).

{/*
FIGURE 18 (figure slot): "Tool Contract Example"
One card showing `place_order(...)` request params and an example JSON response,
plus `poll_fills()` returning a list of fills.
*/}

### 12.7 The Episode (Scenario) and the Task Spec (Prompt)

The example scenario is `take-profit-basic`, defined in `trading-rl-env/tasks/take_profit.py`.

It’s a clean example of the HUD scenario lifecycle:

1. Setup:
   - reset portfolio state with `portfolio.reset(initial_cash=...)`
2. Spec:
   - yield a prompt that defines constraints and success criteria
3. Evaluate:
   - compute a weighted grade from final portfolio state
   - yield a final score in `[0.0, 1.0]`

The prompt is doing real “spec work”, not just instruction writing:

- It explicitly states there is no live price feed.
- It defines the intended price discovery mechanism via fills (`poll_fills()`).
- It lists the available tools as the action space.
- It defines the scoring formula at a high level.

Source of truth:

- `trading-rl-env/tasks/take_profit.py`

{/*
FIGURE 19 (diagram slot): "Take-Profit Scenario Flow"
Prompt -> probe buy -> poll fills -> decide sell -> poll fills -> grade.
*/}

### 12.8 Reward (Grading) in This Environment

This environment uses weighted subscores:

- `PnLGrader` (weight 0.8): realized net profit vs a target, clamped to `[0, 1]`
- `TradeActivityGrader` (weight 0.2): ensures the agent actually traded

Why this is useful:

- “do nothing” gets 0.0
- “tried but failed” can get partial credit (useful shaping signal)
- “met the target” gets 1.0

The grade spec also enforces a correctness constraint:

- weights must sum to `1.0`

Sources of truth:

- `trading-rl-env/grading/graders.py`
- `trading-rl-env/grading/spec.py`

{/*
FIGURE 20 (diagram slot): "Subscores -> Final Score"
Show weights summing to 1.0; call out that this is validated in Grade.score.
*/}

### 12.9 Task Instances (`remote_tasks.json`)

The repo includes a concrete task definition file you can run remotely.

It’s a single JSON object with:

- env name
- scenario id
- args for the scenario

Source of truth:

- `trading-rl-env/remote_tasks.json`

{/*
FIGURE 21 (figure slot): "Trading Task JSON"
Show the `remote_tasks.json` object and label the fields.
*/}

### 12.10 Local Development and Testing (What to Run)

This environment is built to support fast iteration before any platform runs.

Layered integration test:

- `trading-rl-env/test_full.py`
  - Layer 1: REST health (`/api/venuestatus`, `/api/listings`)
  - Layer 2: FIX handshake + basic order/cancel flow
  - Layer 3 (optional): tool layer via `hud dev` on `:8765`
  - Layer 4: grader logic (pure Python)

Local tool probing:

- `trading-rl-env/local_test.py` connects to a running `hud dev` instance and calls tools directly.

Sources of truth:

- `trading-rl-env/test_full.py`
- `trading-rl-env/local_test.py`

Spaces for screenshots (debugging):

{/* SCREENSHOT 13: output of `python trading-rl-env/test_full.py` showing Layer 1/2 passing */}
{/* SCREENSHOT 14: output of Layer 3 tool list via hud dev */}

### 12.11 Deploying the Trading Environment

The trading env README documents deployment via the HUD CLI:

```bash
hud deploy trading-rl-env
```

Source of truth:

- `trading-rl-env/README.md`

In practice, the preferred workflow for most teams is still:

- connect the repo on the HUD platform UI
- let the platform build on pushes
- create tasks from scenarios
- run evals and inspect trajectories

(This post’s deployment section earlier is the general guide; this subsection is just the trading-specific anchor.)

Spaces for screenshots (trading deploy):

{/* SCREENSHOT 15: trading-rl-env environment page on hud.ai */}
{/* SCREENSHOT 16: a run/job results page showing score distribution for take-profit-basic */}

### 12.12 How to Extend This Environment

If you want to add capabilities, the “edit points” are straightforward:

- Add a new tool:
  - create a new module under `trading-rl-env/tools/`
  - register it from `trading-rl-env/env.py`
  - decide what structured JSON it returns (that becomes part of your spec)
- Add a new scenario:
  - create a new file under `trading-rl-env/tasks/`
  - register it from `trading-rl-env/env.py`
  - define setup, prompt/spec, and grading
- Add or adjust grading:
  - implement a new `Grader` in `trading-rl-env/grading/graders.py`
  - compose it into the scenario via `Grade.from_subscores([...])`
- Add tasks:
  - update `trading-rl-env/remote_tasks.json` for remote eval examples
  - or create tasks on the platform from scenarios

If you do one thing after adding a feature: update `test_full.py` so it fails loudly when that feature breaks.

## 13. A Reusable Definition-of-Done Checklist

Use this as your shipping bar for any environment:

- Dockerfile boots the world deterministically.
- env.py registers tools + scenarios and runs over stdio cleanly.
- stdout is reserved for protocol; logs are on stderr.
- Scenarios implement setup -> prompt -> evaluate with clear termination rules.
- Tasks exist as concrete scenario instances (JSON or platform datasets).
- Graders produce reliable, hard-to-game scores (weights validated).
- Trajectories are inspectable for debugging.
- Tests validate dependencies, tools, scenarios, and grading in layers.

That is the work. The rest is just choosing your domain.
